# KNN Model:

#KNN using top 15 most significant variables per F-score criterion. NOT OUR FINAL KNN MODEL.
library('caret')
set.seed(1000)

df1 <- read.csv('/Users/Sunny/Github/CSP571_Movie_Profits_Project/5_merged_with_holidays.csv', stringsAsFactors = T)
colnames(df1)

##Select categorical columns
df2 <- df1[,c(32, 7:24, 30, 34:53)]

##Standardize numerical columns
df3 <- scale(df1[, c(5:6, 27:29, 31, 33)])

##Merge the two dataframes
colnames(df4)
df4 <- cbind(df2, df3)

##Separate the independent variables and the dependent variable
df_ind_col <- df4[,-c(1, 40)]
df_dep_col <- df4[, 1]

##Create a placeholder list to be populated later
var_fstat <- as.numeric(rep(NA, times = ncol(df_ind_col)))

##Names of each list item are the independent variable names
names(var_fstat) <- colnames(df_ind_col)

##Save the F-score of each model after the dependent variable is regressed on each independent variable
for (i in 1:ncol(df_ind_col)){
  var_fstat[i] <- summary(lm(substitute(success_1_to_1 ~ i, 
                                        list(i = as.name(names(var_fstat)[i]))), 
                             data = df4))$fstatistic[[1]]
}

##Sort the scores in descending order
sorted_df <- sort(unlist(var_fstat), decreasing = T)

###############################################################

##Take the top 15 scores to compare to the elastic net variable set
top_15 <- sorted_df[c(1:15)]
names(df_15)

##Select those 15 variables from the primary dataset
df_15 <- df4[, c(1:2, 20, 42:45, 4:7, 9, 15, 17, 19, 21)]
df_15$success_1_to_1 <- as.factor(df_15$success_1_to_1)

##Partition to train/test sets
inTrain <- createDataPartition(y = df_15[,'success_1_to_1'], list = FALSE, p = .8)
train_15 <- df_15[inTrain,]
test_15 <- df_15[-inTrain,]

##Train the model, only k is tuned, distance and kernel are set to their default values
model_rectangular_15 <- train(
  success_1_to_1 ~., 
  data = train_15, 
  method = "kknn",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(kmax = 15:50,
                         distance = 2,
                         kernel = c('rectangular')))

##See results of the trained model and how it did on the validation set during cross validation
plot(model_rectangular_15)
model_rectangular_15$bestTune
max(model_rectangular_15$results$Accuracy)

#KNN using the full set of variables. NOT OUR FINAL KNN MODEL.

library('caret')
set.seed(1000)

df1 <- read.csv('/Users/Sunny/Github/CSP571_Movie_Profits_Project/5_merged_with_holidays.csv', stringsAsFactors = T)

##Convert classification to factors
df1$success_1_to_1 <- as.factor(df1$success_1_to_1)

##Extract categorical variables and other variables that we do not want to standardize
df2 <- df1[,c(32, 7:24, 30, 34:53)]

##Standardize numerical columns
df3 <- scale(df1[, c(5:6, 27:29, 31, 33)])

##Merge the two dataframes
df4 <- cbind(df2, df3)

##Partition into 80%, and 20% test
inTrain <- createDataPartition(y = df4[,'success_1_to_1'], list = FALSE, p = .8)
train <- df4[inTrain,]
test <- df4[-inTrain,]

train_set <- train[, -c(1)]
train_label_class <- train[, 1]
test <- df5[c((splitRow+1):nrow(df5)),]
test_set <- test[, -c(1)]
test_label_class <- test[, 1]

##Error Check for row equality
stopifnot(nrow(train) + nrow(test) == nrow(df4))

##Training the model using the caret package and finding the optimal k for # of neighbors.
##trControl allows the user to control the resampling criteria. We are using 10-fold cross validation.
##tuneGrid allows the user to specify the range of k's to test for the best model

##Rectangular / Non-weighted
model_rectangular <- train(
  success_1_to_1 ~., 
  data = train, 
  method = "kknn",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(kmax = 15:50,
                         distance = 2,
                         kernel = c('rectangular')))
##See best model and its results on the validation set
plot(model_rectangular)
model_rectangular$bestTune
max(model_rectangular$results$Accuracy)

#KNN using elastic net variable set. KNN FINAL MODEL.

library('caret')
set.seed(1000)

##Read in data set with only the elastic net variables remaining
df1_elastic <- read.csv('/Users/Sunny/Github/CSP571_Movie_Profits_Project/elastic_net_final_dataset.csv', stringsAsFactors = T)
df1_elastic <- df1_elastic[2:20]

##Convert classification to factors
df1_elastic$success_1_to_1 <- as.factor(df1_elastic$success_1_to_1)

##Separate the categorical variables
df2_elastic <- df1_elastic[, c(19, 3:11, 15, 17:18)]

##Separate and scale the numerical variables
df3_elastic <- scale(df1_elastic[, c(1:2, 12:14, 16)])

##Recombine the two dataframes
df4_elastic <- cbind(df2_elastic, df3_elastic)

##Partition the train/test set
inTrain <- createDataPartition(y = df4_elastic[,'success_1_to_1'], list = FALSE, p = .8)
train_elastic <- df4_elastic[inTrain,]
test_elastic <- df4_elastic[-inTrain,]

##First model with only k tuning, distance and kernel are set to their defaults. This is for comparision
##against the base model with the full set of variables and with the set of top 15 most significant variables.
model_rectangular_elastic <- train(
  success_1_to_1 ~., 
  data = train_elastic, 
  method = "kknn",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(kmax = 15:50,
                         distance = 2,
                         kernel = c('rectangular')))
##See the results on the validation set
plot(model_rectangular_elastic)
model_rectangular_elastic$bestTune
max(model_rectangular_elastic$results$Accuracy)

##Train model with tuning on K and kernel
model_kernel_elastic <- train(
  success_1_to_1 ~., 
  data = train_elastic, 
  method = "kknn",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(kmax = 15:50,
                         distance = 2,
                         kernel = c('rectangular', 'triangular', 'gaussian', 'epanechnikov')))
##See which hyperparameter values did best and what the results on the validation set were
plot(model_kernel_elastic)
model_kernel_elastic$bestTune
mean(model_kernel_elastic$results$Accuracy)

##Train the model using the best K and kernel hyperparameters (found above) and this time tune distance
##Note: This could all tuning could have bee done in one training model but it was divided up into two
##pieces because runtime was over two hours. This makes it more manageable.
model_gaussian_elastic <- train(
  success_1_to_1 ~., 
  data = train_elastic, 
  method = "kknn",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(kmax = 38,
                         distance = c(1:5),
                         kernel = c('gaussian')))
##See which distance value did best and what the results on the validation set were.
plot(model_gaussian_elastic)
model_gaussian_elastic$bestTune
max(model_gaussian_elastic$results$Accuracy)

##Evaluate the final model on the test set and see its performance statistics.
pred_gaussian_elastic <- predict.train(object = model_gaussian_elastic, test_elastic)
confusionMatrix(pred_gaussian_elastic, reference = test_elastic$success_1_to_1, positive = '1')

# Logistic Regression:

#Reading data from csv file
movie <- read.csv('D:/3rd Sem Courses/Data Preparation and Analysis/Project/CSP571_Movie_Profits_Project-master/final_dataset.csv', header = TRUE, stringsAsFactors = FALSE)
head(movie)

#movie$ScaledProductionBudget <- scale(movie$ProductionBudget)

#movie$Success_2_to_1 <- 0
#movie[,'Success_2_to_1'][movie$earnings_percent > 0] <- 1

#Ranking the quarter variable 
set.seed(123)
library('caret')
movie$quarter <- ifelse(movie$quarter == "Q1",1, ifelse(movie$quarter =="Q2", 2, ifelse(movie$quarter =="Q3", 3, ifelse(movie$quarter == "Q4", 4,movie$quarter))))
movie$quarter <- as.numeric(movie$quarter)

#Converting Scuccess_2_to_1 to factor
movie$Success_2_to_1 <- as.factor(movie$Success_2_to_1)

#We'll do stratified sampling to split our data into training and test sets
targetVar <- 'Success_2_to_1'
movieLR <- movie[, c('actorRank','runtime','ProductionBudget','quarter', 'drama', 'thriller', 'nonfiction', 'action', 'amusement', 'Success_2_to_1')]
inTrain <- createDataPartition(y = movieLR[,targetVar], list = FALSE, p = .8)
trainData <- movieLR[inTrain,]
testData <- movieLR[-inTrain,]
stopifnot(nrow(trainData) + nrow(testData) == nrow(movieLR))

#Logistic Regression on train dataset
logitModel <- glm(Success_2_to_1 ~ .,family=binomial(link='logit'),data=trainData)

#Predict on test data
predicted <- predict(logitModel, testData, type="response")

#Histogram of prediction
hist(predicted)

#testData[,'predicted'] <- predicted

#Deciding optimal prediction probability cutoff for the model
install.packages("InformationValue")
library(InformationValue)
optCutOff <- optimalCutoff(testData$Success_2_to_1, predicted)[1] 

#Model Diagnostics
summary(logitModel)

#Classification of Success or not.
success.pred <- ifelse(predicted > 0.5,1,0)

optCutOff.success.pred <- ifelse(predicted > optCutOff,1,0)

#Claculating the mean of prediction
mean(success.pred)
mean(optCutOff.success.pred)

# Checking classification accuracy
mean(success.pred == testData$Success_2_to_1)

mean(optCutOff.success.pred == testData$Success_2_to_1)  

#Confusion matrix to evaluate how good our results are
confusionMatrix(testData$Success_2_to_1, success.pred)


confusionMatrix(testData$Success_2_to_1, optCutOff.success.pred)


#Missclassification Error
misClassError(testData$Success_2_to_1, predicted, threshold = optCutOff)

#If we take optcutoff/thershold as 0.5
misClassError(testData$Success_2_to_1, predicted, threshold = 0.5)

#The ROC curve
plotROC(testData$Success_2_to_1, predicted)

#install.packages("ROCR")
#library(ROCR)
#pr <- prediction(predicted, testData$Success_2_to_1)
#prf <- performance(pr, measure = "tpr", x.measure = "fpr")
#plot(prf)

#auc <- performance(pr, measure = "auc")
#auc <- auc@y.values[[1]]
#auc

#Calcuting Concordance
Concordance(testData$Success_2_to_1, predicted)

#Ploting precision recall curves
install.packages('DMwR')
library('DMwR')
PRcurve(preds = predicted, trues = testData$Success_2_to_1)

#The deviance
llcomponents <- function(y, predicted.y){
  return(y*log(predicted.y) + (1-y)*log(1-predicted.y))
}

xVars <- c('actorRank', 'runtime','ProductionBudget', 'quarter', 'drama', 'thriller', 'nonfiction', 'action', 'amusement')
y <- trainData[,targetVar]
predicted.y <- predict(logitModel, newdata = trainData[,xVars], type='response')

deviance <- sign(as.numeric(y) - predicted.y)*sqrt(-2*llcomponents(as.numeric(y), predicted.y))

summary(deviance)

# Extract the AIC
aic<- 2 * length(logitModel$coefficients) - 2*logLik(logitModel)
aic
AIC(logitModel)

#Reading data from csv file
movie <- read.csv('/Users/swathi/CSP571_Movie_Profits_Project/5_merged_with_holidays.csv', header = TRUE, stringsAsFactors = FALSE)
head(movie)

#movie$ScaledProductionBudget <- scale(movie$ProductionBudget)

#Ranking the quarter variable 
set.seed(123)
library('caret')
#movie$quarter <- ifelse(movie$quarter == "Q1",1, ifelse(movie$quarter =="Q2", 2, ifelse(movie$quarter =="Q3", 3, ifelse(movie$quarter == "Q4", 4,movie$quarter))))
#movie$quarter <- as.numeric(movie$quarter)

#Checking for outliers
#boxplot(movie$revenue)
#boxplot(movie$budget)
#boxplot(movie$runtime)
#boxplot(movie$directorEarnings)

#Removing outliers
#movie <- movie[-which(movie$revenue %in% boxplot(movie$revenue, plot = FALSE)$out),]
#movie <- movie[-which(movie$budget %in% boxplot(movie$budget, plot = FALSE)$out),]
#movie <- movie[-which(movie$runtime %in% boxplot(movie$runtime, plot = FALSE)$out),]
#movie <- movie[-which(movie$directorEarnings %in% boxplot(movie$directorEarnings, plot = FALSE)$out),]

#Converting Scuccess_1_to_1 to factor
movie$success_1_to_1 <- as.factor(movie$success_1_to_1)

#Scaling
df_continuous <- movie[c("runtime", "revenue", "budget", "directorEarnings")]
df_scaled_cont <- scale(df_continuous) 
movie[c("runtime", "revenue", "budget", "directorEarnings")] <- df_scaled_cont

##Convert date column and sort by date
movie$release_date <- as.Date(movie$release_date)
movie <- movie[order(movie$release_date), -1]

#We'll do stratified sampling to split our data into training and test sets
targetVar <- 'success_1_to_1'
movieLR <- movie[, c('release_date', 'revenue', 'budget', 'runtime', 'animation', 'comedy', 'family', 'adventure', 'fantasy', 'drama', 'romance', 'action', 'crime', 'thriller', 'history', 'mystery', 'music', 'horror', 'war', 'documentary', 'western', 'scifi', 'actorMovieCount', 'directorMovieCount', 'domestic', 'quarter', 'success_1_to_1', 'numOfHolidays', 'newyearsday', 'martinlutherkingjrday', 'valentinesday', 'presidentsday', 'goodfriday', 'mothersday', 'memorialday', 'fathersday', 'independenceday', 'laborday', 'columbusday', 'veteransday', 'thanksgivingday', 'christmaseve', 'christmasday', 'independencedayobserved', 'christmasdayobserved', 'newyearsdayobserved', 'christmaseveobserved')]

inTrain <- createDataPartition(y = movieLR[,targetVar], list = FALSE, p = .8)
trainData <- movieLR[inTrain,]
testData <- movieLR[-inTrain,]
stopifnot(nrow(trainData) + nrow(testData) == nrow(movieLR))

#Logistic Regression on train dataset
logitModel <- glm(success_1_to_1 ~ .,family=binomial(link='logit'),data=trainData, maxit = 100)

#Predict on test data
predicted <- predict(logitModel, testData, type="response")

#Histogram of prediction
hist(predicted)


#Deciding optimal prediction probability cutoff for the model
#install.packages("InformationValue")
library(InformationValue)
optCutOff <- optimalCutoff(testData$success_1_to_1, predicted)[1] 
print(paste0("Optimal cutoff for the model: ", optCutOff))

#Model Diagnostics
summary(logitModel)

#Finding significant variables
importance <- varImp(logitModel, scale = FALSE)
print(importance)

#Classification of Success or not.
success.pred <- ifelse(predicted > 0.5,1,0)

optCutOff.success.pred <- ifelse(predicted > optCutOff,1,0)

#Claculating the mean of prediction
mean(success.pred)
mean(optCutOff.success.pred)

# Checking classification accuracy
mean(success.pred == testData$success_1_to_1)

mean(optCutOff.success.pred == testData$success_1_to_1)  

#Confusion matrix to evaluate how good our results are
check_acc <- as.data.frame(testData$success_1_to_1)
check_acc['actual'] <- as.data.frame(testData$success_1_to_1)
check_acc['prediction'] <- as.factor(success.pred)
caret::confusionMatrix(data=check_acc$prediction, reference=check_acc$actual)

check_acc['opt_prediction'] <- as.factor(optCutOff.success.pred)
caret::confusionMatrix(data=check_acc$opt_prediction, reference=check_acc$actual)

#Missclassification Error
misClassError(testData$success_1_to_1, predicted, threshold = optCutOff)

#If we take optcutoff/thershold as 0.5
misClassError(testData$success_1_to_1, predicted, threshold = 0.5)

#Precision, Recall and F1-Score claculation for thershold 0.5
precision_0.5 <- specificity(testData$success_1_to_1, predicted, threshold = 0.5)
print(paste0("Precision for 0.5 thershold: ", precision_0.5))

recall_0.5 <- sensitivity(testData$success_1_to_1, predicted, threshold = 0.5)
print(paste0("Recall for 0.5 thershold: ", recall_0.5))

F1_score_0.5 <- (2 * precision_0.5 * recall_0.5)/(precision_0.5 + recall_0.5)
print(paste0("F1-Score for 0.5 thershold: ", F1_score_0.5))


#Precision, Recall and F1-Score claculation for thershold optCutOff
precision_optCutOff <- specificity(testData$success_1_to_1, predicted, threshold = optCutOff)
print(paste0("Presicion for optimal cutoff thershold: ", precision_optCutOff))

recall_optCutOff <- sensitivity(testData$success_1_to_1, predicted, threshold = optCutOff)
print(paste0("Recall for optimal cutoff thershold: ", recall_optCutOff))

F1_score_optCutOff <- (2 * precision_optCutOff * recall_optCutOff)/(precision_optCutOff + recall_optCutOff)
print(paste0("F1-Score for optimal cutoff thershold: ", F1_score_optCutOff))

#The ROC curve
plotROC(testData$success_1_to_1, predicted)

#install.packages("ROCR")
#library(ROCR)
#pr <- prediction(predicted, testData$Success_2_to_1)
#prf <- performance(pr, measure = "tpr", x.measure = "fpr")
#plot(prf)

#auc <- performance(pr, measure = "auc")
#auc <- auc@y.values[[1]]
#auc

#Calcuting Concordance
Concordance(testData$success_1_to_1, predicted)

#Ploting precision recall curves
#install.packages('DMwR')
library('DMwR')
PRcurve(preds = predicted, trues = testData$success_1_to_1)

#The deviance
llcomponents <- function(y, predicted.y){
  return(y*log(predicted.y) + (1-y)*log(1-predicted.y))
}

xVars <- c('release_date', 'revenue', 'budget', 'runtime', 'animation', 'comedy', 'family', 'adventure', 'fantasy', 'drama', 'romance', 'action', 'crime', 'thriller', 'history', 'mystery', 'music', 'horror', 'war', 'documentary', 'western', 'scifi', 'actorMovieCount', 'directorMovieCount', 'domestic', 'quarter', 'success_1_to_1', 'numOfHolidays', 'newyearsday', 'martinlutherkingjrday', 'valentinesday', 'presidentsday', 'goodfriday', 'mothersday', 'memorialday', 'fathersday', 'independenceday', 'laborday', 'columbusday', 'veteransday', 'thanksgivingday', 'christmaseve', 'christmasday', 'independencedayobserved', 'christmasdayobserved', 'newyearsdayobserved', 'christmaseveobserved')
y <- trainData[,targetVar]
predicted.y <- predict(logitModel, newdata = trainData[,xVars], type='response')

deviance <- sign(as.numeric(y) - predicted.y)*sqrt(-2*llcomponents(as.numeric(y), predicted.y))

summary(deviance)

# Extract the AIC
aic<- 2 * length(logitModel$coefficients) - 2*logLik(logitModel)
aic
AIC(logitModel)

#install.packages("InformationValue")
#install.packages('DMwR')
set.seed(123)
library('caret')
library('tidyverse')
library(e1071)
library(InformationValue)
library('DMwR')
library(MASS)


#Reading data from csv file
movie <- read.csv('/Users/swath/CSP571_Movie_Profits_Project/elastic_net_final_dataset.csv', header = TRUE, stringsAsFactors = FALSE)
head(movie)
movie$X <- NULL
movie$X.1 <- NULL


#Scaling
df_continuous <- movie[c("runtime", "budget")]
df_scaled_cont <- scale(df_continuous) 
movie[c("runtime", "budget")] <- df_scaled_cont


#We'll do stratified sampling to split our data into training and test sets
targetVar <- 'success_1_to_1'

inTrain <- createDataPartition(y = movie[,targetVar], list = FALSE, p = .8)
trainData <- movie[inTrain,]
testData <- movie[-inTrain,]
stopifnot(nrow(trainData) + nrow(testData) == nrow(movie))


#Logistic Regression on train dataset
#logitModel <- glm(success_1_to_1 ~ .,family=binomial(link='logit'),data=trainData) %>% stepAIC(trace=FALSE)
logitModel <- glm(success_1_to_1 ~ .,family=binomial(link='logit'),data=trainData)

#Model Diagnostics
summary(logitModel)

#Predict on test data
predicted <- predict(logitModel, testData, type="response")

#Histogram of prediction
hist(predicted)


#Deciding optimal prediction probability cutoff for the model
optCutOff <- optimalCutoff(testData$success_1_to_1, predicted)[1] 
print(paste0("Optimal cutoff for the model: ", optCutOff))

#Classification of Success or not.
success.pred <- ifelse(predicted > 0.5,1,0)

optCutOff.success.pred <- ifelse(predicted > optCutOff,1,0)

#Claculating the mean of prediction
mean(success.pred)
mean(optCutOff.success.pred)

# Checking classification accuracy
mean(success.pred == testData$success_1_to_1)

mean(optCutOff.success.pred == testData$success_1_to_1)  

#Confusion matrix to evaluate how good our results are
check_acc <- as.data.frame(as.factor(testData$success_1_to_1))
check_acc['actual'] <- as.data.frame(as.factor(testData$success_1_to_1))
check_acc['prediction'] <- as.factor(success.pred)
caret::confusionMatrix(data=check_acc$prediction, reference=check_acc$actual, positive = "1")


check_acc['opt_prediction'] <- as.factor(optCutOff.success.pred)
caret::confusionMatrix(data=check_acc$opt_prediction, reference=check_acc$actual,  positive = "1")


#Missclassification Error
misClassError(testData$success_1_to_1, predicted, threshold = optCutOff)

#If we take optcutoff/thershold as 0.5
misClassError(testData$success_1_to_1, predicted, threshold = 0.5)

#Precision, Recall and F1-Score claculation for thershold 0.5
confusion_0.5 <- caret::confusionMatrix(data=check_acc$prediction, reference=check_acc$actual, positive = "1")
print(paste0("Precision for 0.5 thershold: ", confusion_0.5$byClass[5]))

print(paste0("Recall for 0.5 thershold: ", confusion_0.5$byClass[6]))

print(paste0("F1-Score for 0.5 thershold: ", confusion_0.5$byClass[7]))


#Precision, Recall and F1-Score claculation for thershold optCutOff
confusion_optCutoff <- caret::confusionMatrix(data=check_acc$opt_prediction, reference=check_acc$actual, positive = "1")
print(paste0("Presicion for optimal cutoff thershold: ", confusion_optCutoff$byClass[5]))

print(paste0("Recall for optimal cutoff thershold: ", confusion_optCutoff$byClass[6]))

print(paste0("F1-Score for optimal cutoff thershold: ", confusion_optCutoff$byClass[7]))


#The ROC curve
plotROC(testData$success_1_to_1, predicted)


#Calcuting Concordance
Concordance(testData$success_1_to_1, predicted)

#Ploting precision recall curves

PRcurve(preds = predicted, trues = testData$success_1_to_1)

#The deviance
llcomponents <- function(y, predicted.y){
  return(y*log(predicted.y) + (1-y)*log(1-predicted.y))
}

xVars <- names(movie)
y <- trainData[,targetVar]
predicted.y <- predict(logitModel, newdata = trainData[,xVars], type='response')

deviance <- sign(as.numeric(y) - predicted.y)*sqrt(-2*llcomponents(as.numeric(y), predicted.y))

summary(deviance)

# Extract the AIC
aic<- 2 * length(logitModel$coefficients) - 2*logLik(logitModel)
aic
AIC(logitModel)










